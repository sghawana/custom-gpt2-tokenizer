{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import heapq\n",
    "import torch\n",
    "\n",
    "def merge_ids(ids: torch.Tensor, pair: tuple[int, int], idx: int) -> torch.Tensor:\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i].item(), ids[i + 1].item()) == pair:\n",
    "            new_ids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            new_ids.append(ids[i].item())\n",
    "            i += 1\n",
    "    return torch.tensor(new_ids, dtype=ids.dtype, device=ids.device)\n",
    "\n",
    "\n",
    "def get_pair_counts(ids: torch.Tensor) -> dict[tuple[int, int], int]:\n",
    "    \n",
    "    pairs = torch.stack((ids[:-1], ids[1:]), dim=1)\n",
    "    pairs_tuple = [tuple(pair.tolist()) for pair in pairs]\n",
    "    \n",
    "    pair_counts = {}\n",
    "    for pair in pairs_tuple:\n",
    "        if pair in pair_counts:\n",
    "            pair_counts[pair] += 1\n",
    "        else:\n",
    "            pair_counts[pair] = 1\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "def generate_merges(ids: torch.Tensor, num_merges: int) -> dict[tuple[int, int], int]:\n",
    "    merges = {}\n",
    "    i = 256\n",
    "    count = 0\n",
    "    pair_count = get_pair_counts(ids)\n",
    "    max_heap = [(-count, pair) for pair, count in pair_count.items()]\n",
    "    heapq.heapify(max_heap)\n",
    "    \n",
    "    while count < num_merges and max_heap:\n",
    "        _, merge_pair = heapq.heappop(max_heap)\n",
    "        merge_pair = tuple(merge_pair)\n",
    "        ids = merge_ids(ids, merge_pair, i)\n",
    "        merges[merge_pair] = i\n",
    "        i += 1\n",
    "        count += 1\n",
    "        pair_count = get_pair_counts(ids)\n",
    "        max_heap = [(-count, pair) for pair, count in pair_count.items()]\n",
    "        heapq.heapify(max_heap)\n",
    "    \n",
    "    return merges\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.vocab = {idx : bytes([idx]) for idx in range(256)}\n",
    "        self.special = None\n",
    "        self.merges = None\n",
    "        self.isTrain = False\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        tokenizer_file = os.path.join(path, \"tokenizer.pkl\")\n",
    "\n",
    "        if not os.path.exists(path) or not os.path.exists(os.path.join(path, \"tokenizer.pkl\")):\n",
    "            raise ValueError(cls.load.__name__ + \": No tokenizer found at the specified directory\")\n",
    "\n",
    "        with open(tokenizer_file, \"rb\") as pkl_file:\n",
    "            return pickle.load(pkl_file)\n",
    "        \n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, \"tokenizer.pkl\"), 'wb') as pkl_file:\n",
    "            pickle.dump(self, pkl_file)\n",
    "            \n",
    "    \n",
    "    def train(self, corpus_path, vocab_size):\n",
    "        \n",
    "        with open(corpus_path) as f: corpus = f.read()\n",
    "        ids = torch.tensor(list(corpus.encode('utf-8', errors='replace')), dtype=torch.int16, device=self.device)\n",
    "        \n",
    "        num_merges = max(0, vocab_size - 256)\n",
    "        self.merges = generate_merges(ids, num_merges)\n",
    "        \n",
    "        for pair, new_idx in self.merges.items():\n",
    "            self.vocab[new_idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.isTrain = True\n",
    "        \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \n",
    "        if self.isTrain == False: print('Warning: Please train the tokenizer first!!!') \n",
    "               \n",
    "        ids = torch.tensor(list(string.encode(\"utf-8\", errors='replace')), dtype=torch.int16, device=self.device)\n",
    "        for pair, target in self.merges.items():\n",
    "            ids = merge_ids(ids, pair, target)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        if self.isTrain == False: print('Warning: Please train the tokenizer first!!!')\n",
    "        \n",
    "        string = \"\"\n",
    "        for token in tokens:\n",
    "            string += self.vocab[token.item].decode('utf-8', errors='replace')\n",
    "        return string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.train('/Users/sam/Desktop/github/word2vec/src/sample.txt', 276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3, 'd': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'a':1, 'b':2, 'c':3}\n",
    "dic['d'] = 4\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 5, 'b': 2, 'c': 3, 'd': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['a'] = 5\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 5, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dic['d']\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n",
      "e\n",
      "b\n",
      "d\n",
      "b\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "graph = {\n",
    "        'a' : ['b', 'c'],\n",
    "        'b' : [ 'd'],\n",
    "        'c' : ['e'],\n",
    "        'd' : [],\n",
    "        'e' : ['b'],\n",
    "        'f' : ['d']  \n",
    "}\n",
    "\n",
    "start_node = 'a'\n",
    "\n",
    "stack = list(start_node)\n",
    "\n",
    "while len(stack) > 0:\n",
    "    node = stack.pop()\n",
    "    print(node)\n",
    "    for val in graph[node]:\n",
    "        stack.append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "graph = {\n",
    "        'a' : ['b', 'c'],\n",
    "        'b' : ['d'],\n",
    "        'c' : ['e'],\n",
    "        'd' : [],\n",
    "        'e' : ['b'],\n",
    "        'f' : ['d']  \n",
    "}\n",
    "\n",
    "from collections import deque\n",
    "def BFS(start_node):\n",
    "    q = deque(start_node)\n",
    "    while len(q) > 0:\n",
    "        node = q.popleft()\n",
    "        print(node)\n",
    "        for val in graph[node]:\n",
    "            q.append(val)\n",
    "\n",
    "\n",
    "\n",
    "start_node = 'd'\n",
    "BFS(start_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
